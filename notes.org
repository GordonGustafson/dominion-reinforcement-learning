* random forest from scratch
* models
parametric models - assume a priori the distribution of each parameter
nonparametric model - no assumption about the distribution of the parameter

* TODO PPO
* TODO Play 50 games against a baseline strategy every N epochs and plot the result
* TODO Why do models seem to stop changing?
** TODO Plot gradient magnitude over time

* REINFORCE with kingdom cards
5 features: "player_vp_lead", "num_provinces_remaining", "max_turns_per_player", "two_provinces_remaining", "one_province_remaining",
6 features: add "average_treasure_value_self"

** 14 features, smithy, village, laboratory, festival, market, 800 games, model chooses only gain actions
add num copper, silver, gold, and smithy, village, laboratory, festival, market owned
*** linear model
***** entropy_loss_weight=0
{'model_chooser': 43.5, 'big_money_provinces_only': 6.5}

*** linear model + 1 hidden layer with 8 nodes
***** entropy_loss_weight=0, epsilon=0.4
{'model_chooser': 31.5, 'big_money_provinces_only': 18.5}
***** entropy_loss_weight=0, epsilon=0.2
{'model_chooser': 37.5, 'big_money_provinces_only': 12.5}
{'model_chooser': 42.5, 'big_money_provinces_only': 7.5}
***** entropy_loss_weight=0, epsilon=0.1
{'model_chooser': 42.0, 'big_money_provinces_only': 8.0}
***** entropy_loss_weight=0, epsilon=0.05


***** entropy_loss_weight=0
{'model_chooser': 45.0, 'big_money_provinces_only': 5.0}
***** entropy_loss_weight=math.exp(-5)
{'model_chooser': 21.0, 'big_money_provinces_only': 29.0}
{'model_chooser': 32.0, 'big_money_provinces_only': 18.0}
***** entropy_loss_weight=math.exp(-4)
{'model_chooser': 43.5, 'big_money_provinces_only': 6.5}
***** entropy_loss_weight=math.exp(-3)
{'model_chooser': 19.0, 'big_money_provinces_only': 31.0}



*** linear model + 1 hidden layer with 16 nodes
***** entropy_loss_weight=0
{'model_chooser': 40.0, 'big_money_provinces_only': 10.0}

*** linear model + 2 hidden layers with 8 nodes
***** entropy_loss_weight=0
{'model_chooser': 37.0, 'big_money_provinces_only': 13.0}

** 6 features, smithy, village, laboratory, festival, market, 800 games, model chooses only gain actions
*** linear model
***** entropy_loss_weight=0
{'model_chooser': 42.0, 'big_money_provinces_only': 8.0}
*** linear model + 1 hidden layer with 8 nodes
***** entropy_loss_weight=0
{'model_chooser': 42.5, 'big_money_provinces_only': 7.5}

** 5 features, smithy, 1600 games, model chooses all actions
*** linear model
{'model_chooser': 44.5, 'big_money_provinces_only': 5.5}
*** linear model + 1 hidden layer with 8 nodes
Didn't buy any smithies in example games!
{'model_chooser': 44.0, 'big_money_provinces_only': 6.0}

** 5 features, smithy, village, laboratory, festival, market, 1600 games, model chooses all actions
*** linear model
**** return entropy for distribution of valid actions
***** entropy_loss_weight=exp(-4)
{'model_chooser': 38.0, 'big_money_provinces_only': 12.0}

***** entropy_loss_weight=exp(-2)
{'model_chooser': 0.0, 'big_money_provinces_only': 50.0}

**** WRONG entropy implementation, returned entropy for selected probability only
***** entropy_loss_weight=exp(-2)
{'model_chooser': 31.0, 'big_money_provinces_only': 19.0}
{'model_chooser': 29.5, 'big_money_provinces_only': 20.5}

***** entropy_loss_weight=exp(-1)
{'model_chooser': 0.0, 'big_money_provinces_only': 50.0}

***** entropy_loss_weight=0
{'model_chooser': 36.5, 'big_money_provinces_only': 13.5}
Gaining too many smithies?
gain silver
gain smithy
gain smithy
play smithy
gain gold
play smithy
gain gold
play smithy
gain gold
gain gold
play smithy
gain gold
gain gold
gain gold
play smithy
gain province
play smithy
gain province
play smithy
gain province
gain duchy
gain duchy
play smithy
gain estate
gain estate
gain province

***** entropy_loss_weight=1, 800 games
{'model_chooser': 0.0, 'big_money_provinces_only': 50.0}

*** linear model + 1 hidden layer with 8 nodes
Didn't buy any smithies in example games!
{'model_chooser': 44.0, 'big_money_provinces_only': 6.0}






* REINFORCE no kingdom cards
** BEST MODEL (don't have weights)
*** 1 feature: num provinces minus 0, 1 linear layer, init bias to zero
**** 1600 iterations with RunningStatisticsNorm1d mean only, momentum=0.0001
{'model_chooser': 45.0, 'big_money_provinces_only': 5.0}
tensor([[ 0.1801],
        [ 0.6483],
        [ 1.4976],
        [-0.9318],
        [-1.2655],
        [-0.3527],
        [ 0.1159],
        [ 0.1595]])
tensor([-1.6654,  0.9505,  1.6536, -1.9874,  0.5069,  7.0223, -3.7058, -1.8533])
*** 
5 input features, 1 hidden layer with 8 nodes, weight_decay=0
batch size of 1 games, 1600 epochs
lr=math.exp(-4)
{'model_chooser': 44.5, 'big_money_provinces_only': 5.5}

** 5 input features, 1 hidden layer with 4 nodes, weight decay=0
*** batch size of 2 games, 200 epochs
**** lr=math.exp(-5)
{'model_chooser': 12.0, 'big_money_provinces_only': 38.0}
**** lr=math.exp(-4)
{'model_chooser': 19.5, 'big_money_provinces_only': 30.5}
**** lr=math.exp(-3)
{'model_chooser': 6.0, 'big_money_provinces_only': 44.0}

*** batch size of 1 games, 800 epochs
**** lr=math.exp(-5)
{'model_chooser': 33.0, 'big_money_provinces_only': 17.0}
{'model_chooser': 34.5, 'big_money_provinces_only': 15.5}

**** lr=math.exp(-4)
{'model_chooser': 42.5, 'big_money_provinces_only': 7.5}
{'model_chooser': 32.0, 'big_money_provinces_only': 18.0}

***** weights of 42.5 win rate
tensor([[-0.1238,  1.1100,  1.2049,  1.0863,  0.4670],
        [ 0.3293,  0.5574,  0.5924, -0.0386, -1.7670],
        [-0.2883,  0.0991, -0.2950,  0.0357, -0.8173],
        [ 0.0288, -0.0805, -0.4803, -0.1319, -0.1190]])
tensor([ 1.4798,  0.1777, -0.3119, -0.6056])
tensor([[-0.9407, -0.2583,  0.2723, -0.0325],
        [ 0.0926,  0.3831,  0.7210,  0.0762],
        [ 0.0711,  1.3759,  0.3733,  0.6104],
        [-0.1944, -0.6855, -0.7602, -0.2211],
        [ 0.7539, -1.0927, -0.3806,  0.0129],
        [ 1.4357,  0.7981,  0.4347,  0.1840],
        [-0.7299, -0.3390, -0.3520,  0.2906],
        [-0.4612,  0.1401, -0.1856, -0.0296]])
tensor([ 0.2531, -0.0576,  0.0478, -0.1689,  0.1228,  0.8175, -1.1676, -0.6896])


**** lr=math.exp(-3)
{'model_chooser': 2.0, 'big_money_provinces_only': 48.0}

*** batch size of 2 games, 400 epochs
**** lr=math.exp(-4)
{'model_chooser': 7.5, 'big_money_provinces_only': 42.5}

*** batch size of 4 games, 200 epochs
**** lr=math.exp(-4)
{'model_chooser': 29.0, 'big_money_provinces_only': 21.0}

*** batch size of 1 games, 1600 epochs
**** lr=math.exp(-4)
{'model_chooser': 43.0, 'big_money_provinces_only': 7.0}
{'model_chooser': 43.0, 'big_money_provinces_only': 7.0}

** 5 input features, 1 hidden layer with 8 nodes, weight_decay=0
*** batch size of 1 games, 1600 epochs
**** lr=math.exp(-4)
{'model_chooser': 44.5, 'big_money_provinces_only': 5.5}
{'model_chooser': 33.0, 'big_money_provinces_only': 17.0}

** 5 input features, 1 hidden layer with 16 nodes, weight_decay=0
*** batch size of 1 games, 1600 epochs
**** lr=math.exp(-4)
{'model_chooser': 40.5, 'big_money_provinces_only': 9.5}
{'model_chooser': 33.0, 'big_money_provinces_only': 17.0}

** 5 input features, 1 hidden layer with 4 nodes, lr=math.exp(-4), weight decay=non-zero
*** batch size of 1 games, 1600 epochs
**** weight_decay=math.exp(-5)
{'model_chooser': 25.5, 'big_money_provinces_only': 24.5}
**** weight_decay=math.exp(-4)
{'model_chooser': 37.5, 'big_money_provinces_only': 12.5}
**** weight_decay=math.exp(-3)
{'model_chooser': 1.5, 'big_money_provinces_only': 48.5}

** 7 input features, lr=math.exp(-4)
*** batch size of 1 games, 800 epochs
**** 1 hidden layer with 8 nodes, ReLU, weight_decay=0
{'model_chooser': 39.5, 'big_money_provinces_only': 10.5}

*** batch size of 1 games, 1600 epochs
**** 1 hidden layer with 8 nodes, ReLU, weight_decay=0
{'model_chooser': 39.0, 'big_money_provinces_only': 11.0}
**** 1 hidden layer with 4 nodes, ReLU, weight_decay=math.exp(-5)
{'model_chooser': 39.0, 'big_money_provinces_only': 11.0}
**** 1 hidden layer with 8 nodes, ReLU, weight_decay=math.exp(-5)
{'model_chooser': 38.5, 'big_money_provinces_only': 11.5}
**** 1 hidden layer with 4 nodes, LeakyRelu, weight_decay=math.exp(-5)
{'model_chooser': 42.5, 'big_money_provinces_only': 7.5}

** 7 input features, weight decay=0, batch size of 1 games, 1600 epochs
*** OneCycleLR(max_lr=math.exp(-1), total_steps=MAX_EPOCHS, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=math.exp(3), final_div_factor=math.exp(5))
{'model_chooser': 0.0, 'big_money_provinces_only': 50.0}

*** OneCycleLR(max_lr=math.exp(-1), total_steps=MAX_EPOCHS, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=math.exp(3), final_div_factor=math.exp(3))
{'model_chooser': 0.0, 'big_money_provinces_only': 50.0}


*** OneCycleLR(max_lr=math.exp(-2), total_steps=MAX_EPOCHS, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=math.exp(2), final_div_factor=math.exp(4))
{'model_chooser': 30.0, 'big_money_provinces_only': 20.0}

*** OneCycleLR(max_lr=math.exp(-2), total_steps=MAX_EPOCHS, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=math.exp(2), final_div_factor=math.exp(3))
{'model_chooser': 30.0, 'big_money_provinces_only': 20.0}

*** OneCycleLR(max_lr=math.exp(-2), total_steps=MAX_EPOCHS, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=math.exp(2), final_div_factor=math.exp(2))
{'model_chooser': 0.0, 'big_money_provinces_only': 50.0}

*** OneCycleLR(max_lr=math.exp(-2), total_steps=MAX_EPOCHS, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=math.exp(2), final_div_factor=math.exp(1))
{'model_chooser': 0.0, 'big_money_provinces_only': 50.0}

*** OneCycleLR(max_lr=math.exp(-2), total_steps=MAX_EPOCHS, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=math.exp(2), final_div_factor=math.exp(0))
{'model_chooser': 23.5, 'big_money_provinces_only': 26.5}


*** OneCycleLR(max_lr=math.exp(-3), total_steps=MAX_EPOCHS, pct_start=0.3, anneal_strategy='cos', cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, div_factor=math.exp(1), final_div_factor=math.exp(1))
{'model_chooser': 17.5, 'big_money_provinces_only': 32.5}

* Learnable Constant only
** 1600 iterations
{'model_chooser': 25.0, 'big_money_provinces_only': 25.0}


* 1 feature: num provinces minus 4, 1 linear layer, init bias to zero
** 1600 iterations
{'model_chooser': 41.5, 'big_money_provinces_only': 8.5}
tensor([[ 0.0543],
        [ 0.6768],
        [ 1.2470],
        [-0.9463],
        [-0.9174],
        [-0.0095],
        [-0.3346],
        [-0.3112]])
tensor([-2.2127,  0.0747,  1.4412, -0.8686,  2.0974,  7.1559, -3.1154, -2.1270])
{'model_chooser': 42.0, 'big_money_provinces_only': 8.0}


** 3200 iterations
{'model_chooser': 39.5, 'big_money_provinces_only': 10.5}
 
** 6400 iterations
{'model_chooser': 44.5, 'big_money_provinces_only': 5.5}
tensor([[-0.2604],
        [ 0.5240],
        [ 2.1226],
        [-1.9065],
        [-1.7552],
        [-0.0930],
        [-1.1947],
        [ 0.2288]])
tensor([-4.9270,  0.3586,  1.7766, -1.6030,  4.0322, 13.8826, -5.7529, -3.6432])

** 1600 iterations
RunningStatisticsNorm1d mean only
{'model_chooser': 39.0, 'big_money_provinces_only': 11.0} momentum: 0.01
{'model_chooser': 40.0, 'big_money_provinces_only': 10.0} momentum: 0.001
{'model_chooser': 44.0, 'big_money_provinces_only': 6.0}  momentum: 0.0001

* 1 feature: num provinces minus 0, 1 linear layer, init bias to zero
** 1600 iterations without normalization
{'model_chooser': 40.5, 'big_money_provinces_only': 9.5}
** 1600 iterations with RunningStatisticsNorm1d mean only, momentum=0.0001
{'model_chooser': 45.0, 'big_money_provinces_only': 5.0}
tensor([[ 0.1801],
        [ 0.6483],
        [ 1.4976],
        [-0.9318],
        [-1.2655],
        [-0.3527],
        [ 0.1159],
        [ 0.1595]])
tensor([-1.6654,  0.9505,  1.6536, -1.9874,  0.5069,  7.0223, -3.7058, -1.8533])

{'model_chooser': 39.0, 'big_money_provinces_only': 11.0}

** 1600 iterations with RunningStatisticsNorm1d mean and variance, momentum=0.0001
{'model_chooser': 39.0, 'big_money_provinces_only': 11.0}


* 4 features: num provinces minus 0, player_vp_lead, one_province_remaining, two_provinces_reamining, 1 linear layer, init bias to zero
** 1600 iterations with RunningStatisticsNorm1d mean only, momentum=0.0001, weight decay exp(-5)
{'model_chooser': 0.0, 'big_money_provinces_only': 50.0}

** 1600 iterations with RunningStatisticsNorm1d mean only, momentum=0.0001, weight decay 0
{'model_chooser': 6.0, 'big_money_provinces_only': 44.0}  # didn't seem to buy gold
{'model_chooser': 25.5, 'big_money_provinces_only': 24.5}
** 3200 iterations with RunningStatisticsNorm1d mean only, momentum=0.0001, weight decay 0
{'model_chooser': 18.5, 'big_money_provinces_only': 31.5}

** 1600 iterations with RunningStatisticsNorm1d mean and variance, momentum=0.0001, weight decay 0
{'model_chooser': 41.0, 'big_money_provinces_only': 9.0}


* 4 features: num provinces minus 0, player_vp_lead, max_turns_per_player, one_province_remaining, two_provinces_reamining, 1 linear layer, init bias to zero
** 1600 iterations with RunningStatisticsNorm1d mean and variance, momentum=0.0001, weight decay 0
*** sum of linear + path with hidden layer width 4 and ReLU
{'model_chooser': 41.0, 'big_money_provinces_only': 9.0}

*** sum of linear + path with hidden layer width 4 and ReLU - 5 features (added turn number)
{'model_chooser': 37.0, 'big_money_provinces_only': 13.0}

*** sum of linear + path with hidden layer width 8 and ReLU - 5 features (added turn number)
{'model_chooser': 37.5, 'big_money_provinces_only': 12.5}

*** sum of linear + path with hidden layer width 8 then hidden layer width 8 with ReLU - 5 features (added turn number)
{'model_chooser': 38.0, 'big_money_provinces_only': 12.0}

*** sum of linear + path with hidden layer width 8 then hidden layer width 8 then hidden layer width 8 with ReLU - 5 features (added turn number)
{'model_chooser': 0.0, 'big_money_provinces_only': 50.0}


* 5 features: num provinces minus 0, player_vp_lead, max_turns_per_player, one_province_remaining, two_provinces_reamining, 1 linear layer, init bias to zero
** 1600 iterations with RunningStatisticsNorm1d mean and variance, momentum=0.0001, weight decay 0
Gamma=1
VP_REWARD_MULTIPLIER gives reward for each action that gains (or loses) vps, multiplied by VP_REWARD_MULTIPLIER
*** VP_REWARD_MULTIPLIER = 0.00
{'model_chooser': 43.0, 'big_money_provinces_only': 7.0}

*** VP_REWARD_MULTIPLIER = 0.005
{'model_chooser': 44.0, 'big_money_provinces_only': 6.0}

*** VP_REWARD_MULTIPLIER = 0.01
{'model_chooser': 43.5, 'big_money_provinces_only': 6.5}

*** VP_REWARD_MULTIPLIER = 0.02
{'model_chooser': 31.0, 'big_money_provinces_only': 19.0}

*** VP_REWARD_MULTIPLIER = 0.04
{'model_chooser': 37.0, 'big_money_provinces_only': 13.0}

*** VP_REWARD_MULTIPLIER = 0.08
{'model_chooser': 41.0, 'big_money_provinces_only': 9.0}

*** VP_REWARD_MULTIPLIER = 0.16
{'model_chooser': 36.0, 'big_money_provinces_only': 14.0}








* 1 feature: max turns per play minus 10, 1 linear layer, init bias randomly (PyTorch default)
** 1600 iterations
{'model_chooser': 34.5, 'big_money_provinces_only': 15.5}
** 3200 iterations
{'model_chooser': 43.0, 'big_money_provinces_only': 7.0}
tensor([[ 0.0920],
        [-1.1036],
        [-1.4966],
        [ 0.6794],
        [ 1.7774],
        [ 1.6055],
        [-0.4822],
        [-0.6847]])
tensor([-2.2165,  1.0692,  3.2071, -2.7148, -0.4532,  7.6236, -4.5398, -2.6700])

** 6400 iterations
{'model_chooser': 39.5, 'big_money_provinces_only': 10.5}
tensor([[ 0.4971],
        [-0.9450],
        [-5.4796],
        [ 1.2017],
        [ 2.2536],
        [ 4.3102],
        [ 0.1191],
        [-0.4202]])
tensor([-2.5928,  3.8580,  4.8222, -2.0722,  2.2610,  7.2872, -7.2139, -0.6373])

** 12800 iterations
{'model_chooser': 39.5, 'big_money_provinces_only': 10.5}
tensor([[-0.1849],
        [-2.3817],
        [-5.9250],
        [ 1.9584],
        [ 2.2022],
        [ 2.2911],
        [ 0.2319],
        [ 1.2373]])
tensor([ -1.5156,  -1.4983,   4.4276,   4.7687,   9.1618,  16.1847, -10.1894,
         -2.9236])




* 2 hidden layers
baseline=0.5
def get_policy_model():
    num_input_features = 7
    hidden_layer_width = 8
    num_model_outputs = NUM_ACTIONS
    return torch.nn.Sequential(
        # torch.nn.BatchNorm1d(num_input_features, affine=False),
        torch.nn.Linear(num_input_features, hidden_layer_width),
        torch.nn.ReLU(),
        torch.nn.Linear(hidden_layer_width, hidden_layer_width),
        torch.nn.ReLU(),
        # torch.nn.BatchNorm1d(hidden_layer_width, affine=True),

        torch.nn.Linear(hidden_layer_width, num_model_outputs, bias=True)
    )


** 3200 games
{'model_chooser': 38.5, 'big_money_provinces_only': 11.5}
** 6400 games
{'model_chooser': 22.5, 'big_money_provinces_only': 27.5}
** 12800 games
{'model_chooser': 19.5, 'big_money_provinces_only': 30.5}





* OLD record for best model ever trained:
{'model_chooser': 79.0, 'big_money_provinces_only': 21.0}
(don't have weights, it was a 4 parameter linear model maybe with bias)

{'model_chooser': 72.0, 'big_money_provinces_only': 28.0}
tensor([[ 1.5439, -0.0056,  1.1046, -1.1007]])

* other models trained
{'model_chooser': 6.0, 'big_money_provinces_only': 94.0}
tensor([[ 0.7510, -0.0781,  0.4192, -0.5021]])
tensor([0.0236])

{'model_chooser': 21.5, 'big_money_provinces_only': 78.5}
tensor([[ 1.0546,  0.1553,  1.3197, -1.4143]])
tensor([0.0009])

{'model_chooser': 22.0, 'big_money_provinces_only': 78.0}
tensor([[ 1.0131,  0.0083,  1.3682, -1.3823]])

{'model_chooser': 38.0, 'big_money_provinces_only': 62.0}
tensor([[ 1.6876,  0.0064,  0.7468, -0.7700]])

{'model_chooser': 55.0, 'big_money_provinces_only': 45.0}
tensor([[ 1.7963,  0.0274,  0.8411, -0.8987]])

{'model_chooser': 50.0, 'big_money_provinces_only': 50.0}
tensor([[ 1.6247, -0.0134,  1.3446, -1.3913]])

** 800 games, 20 epochs per epsilon, epsilons = [1.0, 2**-1, 2**-2, 2**-3, 2**-4]
*** batch size 1024
lr=1e-1, weight_decay=0.04: {'model_chooser': 71.5, 'big_money_provinces_only': 28.5}
lr=1e-1, weight_decay=0.04: {'model_chooser': 80.0, 'big_money_provinces_only': 20.0}
lr=1e-1, weight_decay=0.04: {'model_chooser': 68.0, 'big_money_provinces_only': 32.0}

** 800 games, 20 epochs per epsilon, epsilons = [1.0, 2**-1, 2**-2, 2**-3, 2**-4]
*** batch size 1024
lr=1e-1, weight_decay=0.04: {'model_chooser': 71.5, 'big_money_provinces_only': 28.5}
lr=1e-1, weight_decay=0.04: {'model_chooser': 80.0, 'big_money_provinces_only': 20.0}
lr=1e-1, weight_decay=0.04: {'model_chooser': 68.0, 'big_money_provinces_only': 32.0}

** 400 games, 20 epochs per epsilon, epsilons = [1.0, 2**-1, 2**-2, 2**-3, 2**-4]
*** batch size 1024
lr=1e-1, weight_decay=0.04: {'model_chooser': 77.0, 'big_money_provinces_only': 23.0}
lr=1e-1, weight_decay=0.04: {'model_chooser': 79.5, 'big_money_provinces_only': 20.5}
lr=1e-1, weight_decay=0.04: {'model_chooser': 80.0, 'big_money_provinces_only': 20.0}

** 200 games, 20 epochs per epsilon, epsilons = [1.0, 2**-1, 2**-2, 2**-3, 2**-4]
*** batch size 1024
lr=1e-1, weight_decay=0.04: {'model_chooser': 74.0, 'big_money_provinces_only': 26.0}
lr=1e-1, weight_decay=0.04: {'model_chooser': 79.5, 'big_money_provinces_only': 20.5}
lr=1e-1, weight_decay=0.04: {'model_chooser': 66.5, 'big_money_provinces_only': 33.5}

** 100 games, 20 epochs per epsilon, epsilons = [1.0, 2**-1, 2**-2, 2**-3, 2**-4]
*** batch size 64
lr=1e-1, weight_decay=0.00: {'model_chooser': 11.0, 'big_money_provinces_only': 89.0}

*** batch size 128
lr=1e-1, weight_decay=0.00: {'model_chooser': 62.5, 'big_money_provinces_only': 37.5}

*** batch size 256
lr=1e-1, weight_decay=0.00: {'model_chooser': 61.5, 'big_money_provinces_only': 38.5}
lr=1e-1, weight_decay=0.00: {'model_chooser': 0.0, 'big_money_provinces_only': 100.0}

*** batch size 512
lr=1e-1, weight_decay=0.00: {'model_chooser': 58.0, 'big_money_provinces_only': 42.0}

*** batch size 1024
lr=1e0,  weight_decay=0.01: {'model_chooser': 77.0, 'big_money_provinces_only': 23.0}
lr=1e0,  weight_decay=0.01: {'model_chooser': 0.0, 'big_money_provinces_only': 100.0}

lr=1e-1, weight_decay=0.00: {'model_chooser': 74.0, 'big_money_provinces_only': 26.0}

lr=1e-1, weight_decay=0.01: {'model_chooser': 69.5, 'big_money_provinces_only': 30.5}

lr=1e-1, weight_decay=0.02: {'model_chooser': 2.0, 'big_money_provinces_only': 98.0}

lr=1e-1, weight_decay=0.04: {'model_chooser': 78.0, 'big_money_provinces_only': 22.0}
lr=1e-1, weight_decay=0.04: {'model_chooser': 72.5, 'big_money_provinces_only': 27.5}

lr=1e-1, weight_decay=0.08: {'model_chooser': 47.0, 'big_money_provinces_only': 53.0}
lr=1e-1, weight_decay=0.08: {'model_chooser': 12.5, 'big_money_provinces_only': 87.5}

lr=1e-1, weight_decay=0.16: {'model_chooser': 64.5, 'big_money_provinces_only': 35.5}

lr=1e-1, weight_decay=0.32: {'model_chooser': 71.0, 'big_money_provinces_only': 29.0}
lr=1e-1, weight_decay=0.64: {'model_chooser': 64.0, 'big_money_provinces_only': 36.0}
lr=1e-1, weight_decay=1.28: excessively long games 






** epsilons = [1.0, 2**-1, 2**-2, 2**-3]
*** 1 hidden layer, width 4, Relu -> BatchNorm
lr=1e-2: {'model_chooser': 48.0, 'big_money_provinces_only': 52.0}
lr=1e-1: {'model_chooser': 62.5, 'big_money_provinces_only': 37.5}
lr=1e0: {'model_chooser': 80.5, 'big_money_provinces_only': 19.5}
lr=1e0: {'model_chooser': 70.0, 'big_money_provinces_only': 30.0}
lr=1e0: {'model_chooser': 71.0, 'big_money_provinces_only': 29.0}
lr=1e0: {'model_chooser': 2.0, 'big_money_provinces_only': 98.0}

lr=1e1: {'model_chooser': 0.0, 'big_money_provinces_only': 100.0}

*** 1 hidden layer, width 8, Relu -> BatchNorm
lr=1e0 {'model_chooser': 48.5, 'big_money_provinces_only': 51.5}

*** 1 hidden layer, width 16, Relu -> BatchNorm
{'model_chooser': 0.0, 'big_money_provinces_only': 100.0}

*** 1 hidden layer, width 4, BatchNorm -> Relu
{'model_chooser': 0.0, 'big_money_provinces_only': 100.0}

* TODO implement picking 10 random kingdom cards
* TODO fix bug where game doesn't end if 3 piles are bought out
potentially introduced during switch from dict to Multiset for CardCounts

* cards
** easy to add
    # {"name": "Poacher",      "cost": 4, "type": "action", EFFECT_NAME.DRAW_CARDS: 1, "actions": 1, @"+1$, discard a card per empty supply pile"
    # {"name": "Gardens",      "cost": 4, "type": "victory", @"worth 1 vp per 10 cards you have (rounded down)"
    # {"name": "Throne Room",  "cost": 4, "type": "action", @"you may play an action card from your hand twice"
    # {"name": "Vassal",       "cost": 3, "type": "action", "money_produced": 2, "Discard_the_top_card_of_your_deck_if_it's_an_action_card,_you_may_play_it": 1,
    # {"name": "Artisan",      "cost": 6, "type": "action", @"gain a card to your hand costing up to $5. put a card from your hand onto your deck"
** hard to add
    # {"name": "Merchant",     "cost": 3, "type": "action", EFFECT_NAME.DRAW_CARDS: 1, "actions": 1, "the_first_time_you_play_a_silver_this_turn_+1_money": 1,
    # {"name": "Sentry",       "cost": 5, "type": "action", "actions": 1, @"+1 card . Look at the top 2 cards of your deck. Trash and/or discard any number of them, put the rest back on top in any order"
    # {"name": "Bureaucrat",   "cost": 4, "type": "action", @"gain a silver onto your deck. each other player reveals a victory card from their hand it puts it onto their deck (or reveals a hand with no victory cards)"
    # {"name": "Library",      "cost": 5, "type": "action", @"draw until you have 7 cards in hand, skipping any action cards you choose to. Set those aside, discarding them afterwards"
    # {"name": "Moat",         "cost": 2, "type": "action", EFFECT_NAME.DRAW_CARDS: 2, "moat_effect": 1,


* cards that interact with top of deck
    [3, 0, 0, "Harbinger",    "+1 card, +1 action. Look through your discard pile. you may put a card fram it onto your deck"
    [5, 0, 0, "Sentry",       "+1 card +1 action. Look at the top 2 cards of your deck. Trash and/or discard any number of them, put the rest back on top in any order"
    [4, 0, 0, "Bureaucrat",   "gain a silver onto your deck. each other player reveals a victory card from their hand it puts it onto their deck (or reveals a hand with no victory cards)"

* scratch code
    def non_current_players(self) -> List[Player]:
        result = self.players.copy()
        result.remove(self.current_player())
        return result


    def test_non_current_players(self):
        game_state = make_game_state(turn_phase=TURN_PHASES.CLEANUP,
                                     current_player_index=1,
                                     players=[make_player(name="player at index 0"),
                                              make_player(name="player at index 1"),
                                              make_player(name="player at index 2"),
                                              make_player(name="player at index 3")])

        expected_non_current_players = [make_player(name="player at index 0"),
                                        make_player(name="player at index 2"),
                                        make_player(name="player at index 3")]

        self.assertEqual(game_state.non_current_players(), expected_non_current_players)


* 

card_counts must be a set

a card must be addable and removable from card_counts

must be able to retrieve vp, $, cost, and effects for any card



* 
    {"name": "Cellar",       "cost": 2, "actions": 1, "discard_any_number_then_draw_that_many": 1,
    {"name": "Chapel",       "cost": 2, "trash_up_to_X_cards_from_your_hand": 4,
    {"name": "Moat",         "cost": 2, "draw_cards": 2, "moat_effect": 1,
    {"name": "Harbinger",    "cost": 3, "draw_cards": 1, "actions": 1, "put_any_card_from_discard_pile_onto_deck": 1,
    {"name": "Merchant",     "cost": 3, "draw_cards": 1, "actions": 1, "the_first_time_you_play_a_silver_this_turn_+1_money": 1,
    {"name": "Vassal",       "cost": 3, "money_produced": 2, "Discard_the_top_card_of_your_deck_if_it's_an_action_card,_you_may_play_it": 1,
    {"name": "Village",      "cost": 3, "draw_cards": 1, "actions": 2,
    {"name": "Workshop",     "cost": 3, @"gain_a_card_costing_up_to_4": 1
    {"name": "Bureaucrat",   "cost": 4, @"gain a silver onto your deck. each other player reveals a victory card from their hand it puts it onto their deck (or reveals a hand with no victory cards)"
    {"name": "Militia",      "cost": 4, @"+2$ each other player discards down to 3 cards in hand"
    {"name": "Moneylender",  "cost": 4, @"you may trash a copper from your hand for +3$"
    {"name": "Poacher",      "cost": 4, "draw_cards": 1, "actions": 1, @"+1$, discard a card per empty supply pile"
    {"name": "Remodel",      "cost": 4, @"trash a card from your hand. gain a card costing up to 2 more than it"
    {"name": "Smithy",       "cost": 4, "draw_cards": 3,
    {"name": "Throne Room",  "cost": 4, @"you may play an action card from your hand twice"
    {"name": "Bandit",       "cost": 5, @"gain a gold. each other player reveals the top 2 cards of their deck, trashes a revealed treasure other than copper, and discards the rest"
    {"name": "Council Room", "cost": 5, "draw_cards": 4, @"+1 buy, each other player drawns a card"
    {"name": "Festival",     "cost": 5, "actions": 2, @", +1 buy, +2$"
    {"name": "Laboratory",   "cost": 5, "draw_cards": 2, "actions": 1,
    {"name": "Library",      "cost": 5, @"draw until you have 7 cards in hand, skipping any action cards you choose to. Set those aside, discarding them afterwards"
    {"name": "Market",       "cost": 5, "draw_cards": 1, "actions": 1, @"+1$ +1 buy"
    {"name": "Mine",         "cost": 5, @"you may trash a treasure from your hand. gain a treasure to your hand costing up to $3 more than it"
    {"name": "Sentry",       "cost": 5, "actions": 1, @"+1 card . Look at the top 2 cards of your deck. Trash and/or discard any number of them, put the rest back on top in any order"
    {"name": "Witch",        "cost": 5, "draw_cards": 2, @"each other player gains a curse"
    {"name": "Artisan",      "cost": 6, @"gain a card to your hand costing up to $5. put a card from your hand onto your deck"

* 
 |  fillna(self, value: 'object | ArrayLike | None' = None, method: 'FillnaOptions | None' = None, axis: 'Axis | None' = None, inplace: 'bool' = False, limit=None, downcast=None) -> 'DataFrame | None'
 |      Fill NA/NaN values using the specified method.
 |      
 |      Parameters
 |      ----------
 |      value : scalar, dict, Series, or DataFrame
 |          Value to use to fill holes (e.g. 0), alternately a
 |          dict/Series/DataFrame of values specifying which value to use for
 |          each index (for a Series) or column (for a DataFrame).  Values not
 |          in the dict/Series/DataFrame will not be filled. This value cannot
 |          be a list.
 |      method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None
 |          Method to use for filling holes in reindexed Series
 |          pad / ffill: propagate last valid observation forward to next valid
 |          backfill / bfill: use next valid observation to fill gap.
 |      axis : {0 or 'index', 1 or 'columns'}
 |          Axis along which to fill missing values.
 |      inplace : bool, default False
 |          If True, fill in-place. Note: this will modify any
 |          other views on this object (e.g., a no-copy slice for a column in a
 |          DataFrame).
 |      limit : int, default None
 |          If method is specified, this is the maximum number of consecutive
 |          NaN values to forward/backward fill. In other words, if there is
 |          a gap with more than this number of consecutive NaNs, it will only
 |          be partially filled. If method is not specified, this is the
 |          maximum number of entries along the entire axis where NaNs will be
 |          filled. Must be greater than 0 if not None.
 |      downcast : dict, default is None
 |          A dict of item->dtype of what to downcast if possible,
 |          or the string 'infer' which will try to downcast to an appropriate
 |          equal type (e.g. float64 to int64 if possible).
 |      
 |      Returns
 |      -------
 |      DataFrame or None
 |          Object with missing values filled or None if ``inplace=True``.


* 
    [2, 0, 0, "Cellar",       "+1 action, discard any number then draw that many"
    [2, 0, 0, "Chapel",       "trash up to 4 cards from your hand"
    [2, 0, 0, "Moat",         "+2 cards, whenever another player plays an attack card, you may reveal this from your hand, to be unaffected by it"
    [3, 0, 0, "Harbinger",    "+1 card, +1 action. Look through your discard pile. you may put a card fram it onto your deck"
    [3, 0, 0, "Merchant",     "+1 card, +1 action the first time you play a silver this turn, +1 money"
    [3, 0, 0, "Vassal",       "+2 money. Discard the top card of your deck. if it's an action card, you may play it"
    [3, 0, 0, "Village",      "+1 card, +2 actions"
    [3, 0, 0, "Workshop",     "gain a card costing up to 4"
    [4, 0, 0, "Bureaucrat",   "gain a silver onto your deck. each other player reveals a victory card from their hand it puts it onto their deck (or reveals a hand with no victory cards)"
    [4, 0, 0, "Gardens",      "worth 1 vp per 10 cards you have (rounded down)"
    [4, 0, 0, "Militia",      "+2$ each other player discards down to 3 cards in hand"
    [4, 0, 0, "Moneylender",  "you may trash a copper from your hand for +3$"
    [4, 0, 0, "Poacher",      "+1 card +1 action +1$, discard a card per empty supply pile"
    [4, 0, 0, "Remodel",      "trash a card from your hand. gain a card costing up to 2 more than it"
    [4, 0, 0, "Smithy",       "+3 cards"
    [4, 0, 0, "Throne Room",  "you may play an action card from your hand twice"
    [5, 0, 0, "Bandit",       "gain a gold. each other player reveals the top 2 cards of their deck, trashes a revealed treasure other than copper, and discards the rest"
    [5, 0, 0, "Council Room", "+4 cards +1 buy, each other player drawns a card"
    [5, 0, 0, "Festival",     "+2 actions, +1 buy, +2$"
    [5, 0, 0, "Laboratory",   "+2 cards, +1 action"
    [5, 0, 0, "Library",      "draw until you have 7 cards in hand, skipping any action cards you choose to. Set those aside, discarding them afterwards"
    [5, 0, 0, "Market",       "+1 card +1 action +1$ +1 buy"
    [5, 0, 0, "Mine",         "you may trash a treasure from your hand. gain a treasure to your hand costing up to $3 more than it"
    [5, 0, 0, "Sentry",       "+1 card +1 action. Look at the top 2 cards of your deck. Trash and/or discard any number of them, put the rest back on top in any order"
    [5, 0, 0, "Witch",        "+2 cards, each other player gains a curse"
    [6, 0, 0, "Artisan",      "gain a card to your hand costing up to $5. put a card from your hand onto your deck"



* todos
** TODO work through card implementation list
** TODO do RL hw2

* 
def game_state_to_features(game_state: GameState):
    pass
    # total_victory_points =
    # total_money_for_turn =


24 Estates, 12 Duchies, 12 Provinces
60 copper, 40 silver, 30 gold

2 players: 8 of each Victory card and 10 Curse cards
3 players: 12 of each Victory card and 20 Curse cards
4 players: 12 of each Victory card and 30 Curse cards

* 
2 players	3 players	4 players
copper	46	39	32
silver	40	40	40
gold	    30	30	30
curse	    10	20	30
estate	8	12	12
duchy	    8	12	12
province	8	12	12

The game ends when either 3 Supply piles are empty, or when the Province pile or the Colony pile empties. The player with the most victory points wins.



* dominion base second edition cards
** $2 Cellar:         +1 action, discard any number then draw that many
** $2 Chapel:         trash up to 4 cards from your hand
** $2 Moat:           +2 cards, whenever another player plays an attack card, you may reveal this from your hand, to be unaffected by it
** $3 Harbinger:      +1 card, + 1 action. Look through your discard pile. you may puta card fram it onto your deck
** $3 Merchant:       +1 card, +1 action the first time you play a silver this turn, +1 money
** $3 Vassal:         +2 money. Discard the top card of your deck. if it's an action card, you may play it
** $3 Village:        +1 card, +2 actions
** $3 Workshop:       gain a card costing up to 4
** $4 Bureaucrat:     gain a silver onto your deck. each other player reveals a victory card from their hand it puts it onto their deck (or reveals a hand with no victory cards)
** $4 Gardens:        worth 1 vp per 10 cards you have (rounded down)
** $4 Militia:        +2$ each other player discards down to 3 cards in hand
** $4 Moneylender:    you may trash a copper from your hand for +3$
** $4 Poacher:        +1 card +1 action +1$, discard a card per empty supply pile
** $4 Remodel:        trash a card from your hand. gain a card costing up to 2 more than it
** $4 Smithy:         +3 cards
** $4 Throne Room:    you may play an action card from your hand twice
** $5 Bandit:         gain a gold. each other player reveals the top 2 cards of their deck, trashes a revealed treasure other than copper, and discards the rest
** $5 Council Room:   +4 cards +1 buy, each other player drawns a card
** $5 Festival:       +2 actions, +1 buy, +2$
** $5 Laboratory:     +2 cards, +1 action
** $5 Library:        draw until you have 7 cards in hand, skipping any action cards you choose to. Set those aside, discarding them afterwards
** $5 Market:         +1 card +1 action +1$ +1 buy
** $5 Mine:           you may trash a treasure from your hand. gain a treasure to your hand costing up to $3 more than it.
** $5 Sentry:         +1 card +1 action. Look at the top 2 cards of your deck. Trash and/or discard any number of them, put the rest back on top in any order.
** $5 Witch:          +2 cards, each other player gains a curse
** $6 Artisan:        gain a card to your hand costing up to $5. put a card from your hand onto your deck.

* delete me

def num_provinces(player: Player) -> int:
    return (num_copies_of_card(player.hand, "province")
            + num_copies_of_card(player.deck, "province")
            + num_copies_of_card(player.discard_pile, "province"))


def test_num_provinces(self):
    player = Player(hand=dict_to_card_counts({"estate": 2, "copper": 3, "province": 1}),
                    deck=dict_to_card_counts({"copper": 4, "province": 2}),
                    discard_pile=dict_to_card_counts({"province": 3}))

    self.assertEqual(num_provinces(player), 6)
